import streamlit as st
import openai
import os
from pathlib import Path
import zipfile
import chardet  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡ºãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from aozora_preprocess import save_cleanse_text  # å‰å‡¦ç†ã®é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

author_id = '000129'  # é’ç©ºæ–‡åº«ã®ä½œå®¶ç•ªå·
author_name = 'æ£®é´å¤–'  # é’ç©ºæ–‡åº«ã®è¡¨è¨˜ã§ã®ä½œå®¶å

# ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£å‡ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°
@st.cache_data
def load_all_texts_from_zip(zip_file):
    all_texts = ""
    unzip_dir = Path("unzipped_files")
    unzip_dir.mkdir(exist_ok=True)

    with zipfile.ZipFile(zip_file, 'r') as zip_ref:
        zip_ref.extractall(unzip_dir)  # è§£å‡å…ˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    text_files = list(unzip_dir.glob('**/*.txt'))
    for file_path in text_files:
        # ã¾ãšãƒã‚¤ãƒˆå½¢å¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º
        with open(file_path, 'rb') as f:
            raw_data = f.read()
            result = chardet.detect(raw_data)
            encoding = result['encoding']  # æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—

        try:
            with open(file_path, "r", encoding=encoding) as f:
                all_texts += f.read() + "\n"
        except UnicodeDecodeError:
            st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    return all_texts

# ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹é–¢æ•°
def process_text_files():
    processed_texts = []  # å‡¦ç†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    unzip_dir = Path("unzipped_files")
    text_files = list(unzip_dir.glob('**/*.txt'))  # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚‚å«ã‚€

    for text_file in text_files:
        cleaned_df = save_cleanse_text(text_file, unzip_dir)  # å‰å‡¦ç†é–¢æ•°ã‚’å‘¼ã³å‡ºã—
        if cleaned_df is not None:
            # æ•´å½¢å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
            processed_texts.append(cleaned_df.to_string(index=False))

    return processed_texts

# ã™ã¹ã¦ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã‚€
zip_files_directory = Path("000129/files")
zip_files = list(zip_files_directory.glob('*.zip'))  # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—

# å…¨ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã™ã¹ã¦ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦å‡¦ç†ã‚’è¡Œã†ï¼‰
all_processed_texts = []
for zip_file_path in zip_files:
    load_all_texts_from_zip(zip_file_path)  # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    processed_texts = process_text_files()  # ãƒ†ã‚­ã‚¹ãƒˆã®å‡¦ç†
    all_processed_texts.extend(processed_texts)  # ã™ã¹ã¦ã®å‡¦ç†ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 

# æ•´å½¢å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
st.text_area("æ•´å½¢å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿", "\n\n".join(all_processed_texts), height=300)

# Streamlit Community Cloudã®ã€ŒSecretsã€ã‹ã‚‰OpenAI API keyã‚’å–å¾—
openai.api_key = st.secrets.OpenAIAPI.openai_api_key

# st.session_stateã‚’ä½¿ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚„ã‚Šã¨ã‚Šã‚’ä¿å­˜
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "system", "content": st.secrets.AppSettings.chatbot_setting} 
    ]

# ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¨ã‚„ã‚Šã¨ã‚Šã™ã‚‹é–¢æ•°
def communicate():
    messages = st.session_state["messages"]
    user_message = {"role": "user", "content": st.session_state["user_input"]}
    messages.append(user_message)

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages
    )

    bot_message = response["choices"][0]["message"]
    messages.append(bot_message)

    st.session_state["user_input"] = ""  # å…¥åŠ›æ¬„ã‚’ã‚¯ãƒªã‚¢

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹
st.title(author_name+"ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
st.write(author_name+"ã®ä½œå“ã«åŸºã¥ã„ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ›
user_input = st.text_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", key="user_input", on_change=communicate)

if st.session_state["messages"]:
    messages = st.session_state["messages"]
    for message in reversed(messages[1:]):  # ç›´è¿‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸Šã«
        speaker = "ğŸ™‚" if message["role"] == "user" else "ğŸ¤–"
        st.write(speaker + ": " + message["content"])

# æ•´å½¢å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
processed_texts = process_text_files()
for i, text in enumerate(processed_texts):
    st.text_area(f"æ•´å½¢å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ {i+1}", text, height=300)
